{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.model_selection import KFold\n",
                "from sklearn.metrics import accuracy_score, f1_score\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.model_selection import train_test_split\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "def log_entropy_weight(matrix):\n",
                "    if type(matrix) is not np.ndarray:\n",
                "        matrix = matrix.toarray()\n",
                "    normalized = matrix / (1 + np.sum(matrix, axis=0))\n",
                "    nr_docs, _ = matrix.shape\n",
                "    '''\n",
                "        g_i = 1 + sum     p_ij * log(p_ij + 1)   \n",
                "                 j=1,N  ------------------------\n",
                "                               log(N)                              \n",
                "    '''\n",
                "    entropy = 1 + np.sum(np.multiply(normalized, np.log(normalized + 1)), axis=0)/np.log(nr_docs)\n",
                "    '''\n",
                "        logent_ij = gi * log(tf_ij + 1)\n",
                "    '''\n",
                "    log_ent = entropy * np.log(matrix + 1)\n",
                "    return log_ent\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "langs = [\"nl\",\"da\",\"de\",\"it\",\"es\",]\n",
                "\n",
                "model = LogisticRegression(penalty='l2',\n",
                "                           dual=False,\n",
                "                           max_iter=10000,\n",
                "                           tol=0.0001,\n",
                "                           solver='liblinear',\n",
                "                           C=1,\n",
                "                           fit_intercept=True,\n",
                "                           intercept_scaling=1.0,\n",
                "                           class_weight=None,\n",
                "                           random_state=1)\n",
                "\n",
                "cvc = CountVectorizer(max_features=500,\n",
                "                      ngram_range=(1, 5))\n",
                "\n",
                "ans = pd.DataFrame()\n",
                "\n",
                "\n",
                "def simpleModel(X_train, y_train, X_test):\n",
                "    print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
                "    X = cvc.fit_transform(list(X_train) + list(X_test))\n",
                "    X = log_entropy_weight(X)\n",
                "    model.fit(X[:len(X_train)], y_train)\n",
                "    res = model.predict(X[len(X_train):])\n",
                "    return res\n",
                "\n",
                "acc = []\n",
                "f1 = []"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "sentences of 0 <= len < 2\n",
                        "Train size: 2298, Test size: 809\n",
                        "Train size: 1723, Test size: 575\n",
                        "acc: 0.568695652173913\n",
                        "f1: 0.5959924711035276\n",
                        "sentences of 2 <= len < 5\n",
                        "Train size: 8023, Test size: 2796\n",
                        "Train size: 6017, Test size: 2006\n",
                        "acc: 0.5438683948155534\n",
                        "f1: 0.5705014152665536\n",
                        "sentences of 5 <= len < 10\n",
                        "Train size: 11417, Test size: 3594\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn [4], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m train_data \u001b[39m=\u001b[39m all_train_data\u001b[39m.\u001b[39mloc[all_train_data\u001b[39m.\u001b[39msencount\u001b[39m.\u001b[39mbetween(l, r, inclusive\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m\"\u001b[39m)]\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m     15\u001b[0m test_data \u001b[39m=\u001b[39m all_test_data\u001b[39m.\u001b[39mloc[all_test_data\u001b[39m.\u001b[39msencount\u001b[39m.\u001b[39mbetween(l, r, inclusive\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m\"\u001b[39m)]\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m---> 17\u001b[0m res \u001b[39m=\u001b[39m simpleModel(train_data\u001b[39m.\u001b[39;49mtext, train_data\u001b[39m.\u001b[39;49mlabel, test_data\u001b[39m.\u001b[39;49mtext)\n\u001b[0;32m     18\u001b[0m test_data[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m res\n\u001b[0;32m     19\u001b[0m test_data \u001b[39m=\u001b[39m test_data\u001b[39m.\u001b[39mdrop(\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
                        "Cell \u001b[1;32mIn [3], line 22\u001b[0m, in \u001b[0;36msimpleModel\u001b[1;34m(X_train, y_train, X_test)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msimpleModel\u001b[39m(X_train, y_train, X_test):\n\u001b[0;32m     21\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTrain size: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(X_train)\u001b[39m}\u001b[39;00m\u001b[39m, Test size: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(X_test)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m     X \u001b[39m=\u001b[39m cvc\u001b[39m.\u001b[39;49mfit_transform(\u001b[39mlist\u001b[39;49m(X_train) \u001b[39m+\u001b[39;49m \u001b[39mlist\u001b[39;49m(X_test))\n\u001b[0;32m     23\u001b[0m     X \u001b[39m=\u001b[39m log_entropy_weight(X)\n\u001b[0;32m     24\u001b[0m     model\u001b[39m.\u001b[39mfit(X[:\u001b[39mlen\u001b[39m(X_train)], y_train)\n",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py:1354\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmax_df corresponds to < documents than min_df\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1353\u001b[0m \u001b[39mif\u001b[39;00m max_features \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1354\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sort_features(X, vocabulary)\n\u001b[0;32m   1355\u001b[0m X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_words_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_limit_features(\n\u001b[0;32m   1356\u001b[0m     X, vocabulary, max_doc_count, min_doc_count, max_features\n\u001b[0;32m   1357\u001b[0m )\n\u001b[0;32m   1358\u001b[0m \u001b[39mif\u001b[39;00m max_features \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\feature_extraction\\text.py:1146\u001b[0m, in \u001b[0;36mCountVectorizer._sort_features\u001b[1;34m(self, X, vocabulary)\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[39mfor\u001b[39;00m new_val, (term, old_val) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(sorted_features):\n\u001b[0;32m   1145\u001b[0m     vocabulary[term] \u001b[39m=\u001b[39m new_val\n\u001b[1;32m-> 1146\u001b[0m     map_index[old_val] \u001b[39m=\u001b[39m new_val\n\u001b[0;32m   1148\u001b[0m X\u001b[39m.\u001b[39mindices \u001b[39m=\u001b[39m map_index\u001b[39m.\u001b[39mtake(X\u001b[39m.\u001b[39mindices, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mclip\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1149\u001b[0m \u001b[39mreturn\u001b[39;00m X\n",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "all_train_data = pd.DataFrame()\n",
                "all_test_data = pd.DataFrame()\n",
                "\n",
                "for language in langs:\n",
                "    all_train_data = pd.concat([all_train_data, pd.read_csv(f\"corpus/train/{language}/{language}_noent.csv\")])\n",
                "    all_test_data = pd.concat([all_test_data, pd.read_csv(f\"corpus/test/{language}/{language}_noent.csv\")])\n",
                "\n",
                "all_train_data[\"sencount\"] = all_train_data.text.str.count(\"SENTSEP\")\n",
                "all_test_data[\"sencount\"] = all_test_data.text.str.count(\"SENTSEP\")\n",
                "\n",
                "max_sen_count = max(all_train_data[\"sencount\"].max(), all_test_data[\"sencount\"].max())\n",
                "for l,r in [(0,2), (2,5), (5, 10), (10, 1000)]:\n",
                "    print(f\"sentences of {l} <= len < {r}\")\n",
                "    train_data = all_train_data.loc[all_train_data.sencount.between(l, r, inclusive=\"left\")].copy()\n",
                "    test_data = all_test_data.loc[all_test_data.sencount.between(l, r, inclusive=\"left\")].copy()\n",
                "\n",
                "    res = simpleModel(train_data.text, train_data.label, test_data.text)\n",
                "    test_data[\"label\"] = res\n",
                "    test_data = test_data.drop(\"text\", axis=1)\n",
                "    ans = pd.concat([ans, test_data])\n",
                "\n",
                "    X_train, X_test, y_train, y_test = train_test_split(train_data.text, train_data.label, test_size=0.25, random_state=21, shuffle=True)\n",
                "    res = simpleModel(X_train, y_train, X_test)\n",
                "    acc.append(accuracy_score(res, y_test))\n",
                "    f1.append(f1_score(res, y_test,  average='weighted'))\n",
                "    print(\"acc:\", acc[-1])\n",
                "    print(\"f1:\", f1[-1])\n",
                "\n",
                "print(np.round(np.mean(acc), 2))\n",
                "print(np.round(np.mean(f1), 2))\n",
                "results = ans.copy()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "   id     label\n",
                        "0   1   England\n",
                        "0   2  Scotland\n",
                        "1   3   England\n",
                        "0   4   England\n",
                        "1   5  Scotland\n"
                    ]
                }
            ],
            "source": [
                "ans[\"index\"] += 1\n",
                "ans = ans.sort_values(by=[\"index\"])\n",
                "ans = ans.rename(columns={\"index\": \"id\"})\n",
                "ans = ans.drop(\"sencount\", axis=1)\n",
                "print(ans.head())\n",
                "ans.to_csv(\"idc_lang_n-words.csv\",index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# kfold = KFold(n_splits=5, shuffle=True, random_state=21)\n",
                "\n",
                "# for language in langs:\n",
                "#     print(\"working on:\", language)\n",
                "#     train = pd.read_csv(f\"corpus/train/{language}/{language}_noent.csv\")\n",
                "#     acc = []\n",
                "#     f1 = []\n",
                "#     for train_index, test_index in kfold.split(train):\n",
                "#         X_train, X_test = train.text[train_index], train.text[test_index]\n",
                "#         y_train, y_test = train.label[train_index], train.label[test_index]\n",
                "\n",
                "#         X = cvc.fit_transform(list(X_train) + list(X_test))\n",
                "#         X = log_entropy_weight(X)\n",
                "\n",
                "#         model.fit(X[:len(X_train)], y_train)\n",
                "#         res = model.predict(X[len(X_train):])\n",
                "#         acc.append(accuracy_score(res, y_test))\n",
                "#         f1.append(f1_score(res, y_test, average='weighted'))\n",
                "#     print(\"Acc:\", acc)\n",
                "#     print(\"F1: \", f1)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.10.2 64-bit",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.2"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
