{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.model_selection import KFold\n",
                "from sklearn.metrics import accuracy_score, f1_score\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "def log_entropy_weight(matrix):\n",
                "    if type(matrix) is not np.ndarray:\n",
                "        matrix = matrix.toarray()\n",
                "    normalized = matrix / (1 + np.sum(matrix, axis=0))\n",
                "    nr_docs, _ = matrix.shape\n",
                "    '''\n",
                "        g_i = 1 + sum     p_ij * log(p_ij + 1)   \n",
                "                 j=1,N  ------------------------\n",
                "                               log(N)                              \n",
                "    '''\n",
                "    entropy = 1 + np.sum(np.multiply(normalized, np.log(normalized + 1)), axis=0)/np.log(nr_docs)\n",
                "    '''\n",
                "        logent_ij = gi * log(tf_ij + 1)\n",
                "    '''\n",
                "    log_ent = entropy * np.log(matrix + 1)\n",
                "    return log_ent\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "langs = [\"nl\",\"da\",\"de\",\"it\",\"es\",]\n",
                "\n",
                "model = LogisticRegression(penalty='l2',\n",
                "                           dual=False,\n",
                "                           max_iter=10000,\n",
                "                           tol=0.0001,\n",
                "                           solver='liblinear',\n",
                "                           C=1,\n",
                "                           fit_intercept=True,\n",
                "                           intercept_scaling=1.0,\n",
                "                           class_weight=None,\n",
                "                           random_state=1)\n",
                "\n",
                "cvc = CountVectorizer(max_features=2000,\n",
                "                      strip_accents='unicode',\n",
                "                      ngram_range=(1, 5))\n",
                "\n",
                "ans = pd.DataFrame()\n",
                "\n",
                "\n",
                "def simpleModel(X_train, y_train, X_test):\n",
                "    print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
                "    X = cvc.fit_transform(list(X_train) + list(X_test))\n",
                "    X = log_entropy_weight(X)\n",
                "    model.fit(X[:len(X_train)], y_train)\n",
                "    res = model.predict(X[len(X_train):])\n",
                "    return res\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "working on: nl\n",
                        "sentences of 0 <= len < 1000\n",
                        "Train size: 8313, Test size: 2772\n",
                        "working on: da\n",
                        "sentences of 0 <= len < 1000\n",
                        "Train size: 8314, Test size: 2772\n",
                        "working on: de\n",
                        "sentences of 0 <= len < 1000\n",
                        "Train size: 8313, Test size: 2772\n",
                        "working on: it\n",
                        "sentences of 0 <= len < 1000\n",
                        "Train size: 8314, Test size: 2772\n",
                        "working on: es\n",
                        "sentences of 0 <= len < 1000\n",
                        "Train size: 8314, Test size: 2772\n"
                    ]
                }
            ],
            "source": [
                "for language in langs:\n",
                "    print(\"working on:\", language)\n",
                "    all_train_data = pd.read_csv(f\"../corpus/train/{language}/{language}_pos.csv\")\n",
                "    all_test_data = pd.read_csv(f\"../corpus/test/{language}/{language}_pos.csv\")\n",
                "\n",
                "    all_train_data[\"sencount\"] = all_train_data.text.str.count(\"SENTSEP\")\n",
                "    all_test_data[\"sencount\"] = all_test_data.text.str.count(\"SENTSEP\")\n",
                "    max_sen_count = max(all_train_data[\"sencount\"].max(), all_test_data[\"sencount\"].max())\n",
                "    for l, r in [(0, 1000)]:\n",
                "        print(f\"sentences of {l} <= len < {r}\")\n",
                "        train_data = all_train_data.loc[all_train_data.sencount.between(l, r, inclusive=\"left\")].copy()\n",
                "        test_data = all_test_data.loc[all_test_data.sencount.between(l, r, inclusive=\"left\")].copy()\n",
                "\n",
                "        res = simpleModel(train_data.text, train_data.label, test_data.text)\n",
                "        \n",
                "        test_data[\"label\"] = res\n",
                "        test_data = test_data.drop(\"text\", axis=1)\n",
                "        ans = pd.concat([ans, test_data])\n",
                "results = ans.copy()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "   id     label\n",
                        "0   1   England\n",
                        "0   2   England\n",
                        "1   3   England\n",
                        "0   4  Scotland\n",
                        "1   5  Scotland\n"
                    ]
                }
            ],
            "source": [
                "ans[\"index\"] += 1\n",
                "ans = ans.sort_values(by=[\"index\"])\n",
                "ans = ans.rename(columns={\"index\": \"id\"})\n",
                "ans = ans.drop(\"sencount\", axis=1)\n",
                "print(ans.head())\n",
                "ans.to_csv(\"../submissions/Regression_log-entropy_pos.csv\",index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "working on: nl\n",
                        "Train size: 6650, Test size: 1663\n",
                        "Train size: 6650, Test size: 1663\n",
                        "Train size: 6650, Test size: 1663\n",
                        "Train size: 6651, Test size: 1662\n",
                        "Train size: 6651, Test size: 1662\n",
                        "Acc: [0.49188214070956104, 0.49188214070956104, 0.4834636199639206, 0.5096269554753309, 0.5006016847172082]\n",
                        "F1:  [0.49766950430433, 0.49810022050823727, 0.48963373028580487, 0.5161640848417768, 0.5057445474199934]\n",
                        "working on: da\n",
                        "Train size: 6651, Test size: 1663\n",
                        "Train size: 6651, Test size: 1663\n",
                        "Train size: 6651, Test size: 1663\n",
                        "Train size: 6651, Test size: 1663\n",
                        "Train size: 6652, Test size: 1662\n",
                        "Acc: [0.49969933854479853, 0.523752254960914, 0.4954900781719784, 0.5063138905592303, 0.52647412755716]\n",
                        "F1:  [0.508251380928599, 0.5341123855336218, 0.5018842873692266, 0.5105248653177683, 0.5302379486557843]\n",
                        "working on: de\n",
                        "Train size: 6650, Test size: 1663\n",
                        "Train size: 6650, Test size: 1663\n",
                        "Train size: 6650, Test size: 1663\n",
                        "Train size: 6651, Test size: 1662\n",
                        "Train size: 6651, Test size: 1662\n",
                        "Acc: [0.4924834636199639, 0.50210463018641, 0.49969933854479853, 0.49879663056558365, 0.51985559566787]\n",
                        "F1:  [0.4993289550459686, 0.506582170779675, 0.511727606996358, 0.5016483242853055, 0.5258608843399193]\n",
                        "working on: it\n",
                        "Train size: 6651, Test size: 1663\n",
                        "Train size: 6651, Test size: 1663\n",
                        "Train size: 6651, Test size: 1663\n",
                        "Train size: 6651, Test size: 1663\n",
                        "Train size: 6652, Test size: 1662\n",
                        "Acc: [0.5063138905592303, 0.5015033072760072, 0.4924834636199639, 0.4834636199639206, 0.5210589651022864]\n",
                        "F1:  [0.5138196420711492, 0.505043162805663, 0.49838642911461734, 0.48644741127862506, 0.5243721369931068]\n",
                        "working on: es\n",
                        "Train size: 6651, Test size: 1663\n",
                        "Train size: 6651, Test size: 1663\n",
                        "Train size: 6651, Test size: 1663\n",
                        "Train size: 6651, Test size: 1663\n",
                        "Train size: 6652, Test size: 1662\n"
                    ]
                }
            ],
            "source": [
                "kfold = KFold(n_splits=5, shuffle=True, random_state=21)\n",
                "\n",
                "for language in langs:\n",
                "    print(\"working on:\", language)\n",
                "    train = pd.read_csv(f\"../corpus/train/{language}/{language}_pos.csv\")\n",
                "    acc = []\n",
                "    f1 = []\n",
                "    for train_index, test_index in kfold.split(train):\n",
                "        X_train, X_test = train.text[train_index], train.text[test_index]\n",
                "        y_train, y_test = list(train.label[train_index]), list(train.label[test_index])\n",
                "\n",
                "        res = simpleModel(X_train, y_train, X_test)\n",
                "\n",
                "        acc.append(accuracy_score(res, y_test))\n",
                "        f1.append(f1_score(res, y_test, average=\"weighted\"))\n",
                "\n",
                "    print(\"Acc:\", acc)\n",
                "    print(\"F1: \", f1)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.10.2 64-bit",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.2"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
