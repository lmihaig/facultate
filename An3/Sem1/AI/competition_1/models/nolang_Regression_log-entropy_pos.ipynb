{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.model_selection import KFold\n",
                "from sklearn.metrics import accuracy_score, f1_score\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.model_selection import train_test_split\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "def log_entropy_weight(matrix):\n",
                "    if type(matrix) is not np.ndarray:\n",
                "        matrix = matrix.toarray()\n",
                "    normalized = matrix / (1 + np.sum(matrix, axis=0))\n",
                "    nr_docs, _ = matrix.shape\n",
                "    '''\n",
                "        g_i = 1 + sum     p_ij * log(p_ij + 1)   \n",
                "                 j=1,N  ------------------------\n",
                "                               log(N)                              \n",
                "    '''\n",
                "    entropy = 1 + np.sum(np.multiply(normalized, np.log(normalized + 1)), axis=0)/np.log(nr_docs)\n",
                "    '''\n",
                "        logent_ij = gi * log(tf_ij + 1)\n",
                "    '''\n",
                "    log_ent = entropy * np.log(matrix + 1)\n",
                "    return log_ent\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "langs = [\"nl\",\"da\",\"de\",\"it\",\"es\",]\n",
                "\n",
                "model = LogisticRegression(penalty='l2',\n",
                "                           dual=False,\n",
                "                           max_iter=10000,\n",
                "                           tol=0.0001,\n",
                "                           solver='liblinear',\n",
                "                           C=1,\n",
                "                           fit_intercept=True,\n",
                "                           intercept_scaling=1.0,\n",
                "                           class_weight=None,\n",
                "                           random_state=1)\n",
                "\n",
                "cvc = CountVectorizer(max_features=2000,\n",
                "                      ngram_range=(1, 5))\n",
                "\n",
                "ans = pd.DataFrame()\n",
                "\n",
                "\n",
                "def simpleModel(X_train, y_train, X_test):\n",
                "    print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
                "    X = cvc.fit_transform(list(X_train) + list(X_test))\n",
                "    X = log_entropy_weight(X)\n",
                "    model.fit(X[:len(X_train)], y_train)\n",
                "    res = model.predict(X[len(X_train):])\n",
                "    return res\n",
                "\n",
                "acc = []\n",
                "f1 = []"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "sentences of 0 <= len < 10\n",
                        "Train size: 16261, Test size: 5511\n",
                        "Train size: 12195, Test size: 4066\n",
                        "acc: 0.5122970978848992\n",
                        "f1: 0.5242117554489256\n",
                        "sentences of 10 <= len < 1000\n",
                        "Train size: 25307, Test size: 8349\n",
                        "Train size: 18980, Test size: 6327\n",
                        "acc: 0.5621937727200885\n",
                        "f1: 0.5983211106430901\n",
                        "0.54\n",
                        "0.56\n"
                    ]
                }
            ],
            "source": [
                "all_train_data = pd.DataFrame()\n",
                "all_test_data = pd.DataFrame()\n",
                "\n",
                "for language in langs:\n",
                "    all_train_data = pd.concat([all_train_data, pd.read_csv(f\"corpus/train/{language}/{language}_pos.csv\")])\n",
                "    all_test_data = pd.concat([all_test_data, pd.read_csv(f\"corpus/test/{language}/{language}_pos.csv\")])\n",
                "\n",
                "all_train_data[\"sencount\"] = all_train_data.text.str.count(\"SENTSEP\")\n",
                "all_test_data[\"sencount\"] = all_test_data.text.str.count(\"SENTSEP\")\n",
                "\n",
                "max_sen_count = max(all_train_data[\"sencount\"].max(), all_test_data[\"sencount\"].max())\n",
                "for l, r in [(0, 10), (10, 1000)]:\n",
                "    print(f\"sentences of {l} <= len < {r}\")\n",
                "    train_data = all_train_data.loc[all_train_data.sencount.between(l, r, inclusive=\"left\")].copy()\n",
                "    test_data = all_test_data.loc[all_test_data.sencount.between(l, r, inclusive=\"left\")].copy()\n",
                "\n",
                "    res = simpleModel(train_data.text, train_data.label, test_data.text)\n",
                "    test_data[\"label\"] = res\n",
                "    test_data = test_data.drop(\"text\", axis=1)\n",
                "    ans = pd.concat([ans, test_data])\n",
                "\n",
                "    X_train, X_test, y_train, y_test = train_test_split(train_data.text, train_data.label, test_size=0.25, random_state=21, shuffle=True)\n",
                "    res = simpleModel(X_train, y_train, X_test)\n",
                "    acc.append(accuracy_score(res, y_test))\n",
                "    f1.append(f1_score(res, y_test,  average='weighted'))\n",
                "    print(\"acc:\", acc[-1])\n",
                "    print(\"f1:\", f1[-1])\n",
                "\n",
                "print(np.round(np.mean(acc), 2))\n",
                "print(np.round(np.mean(f1), 2))\n",
                "results = ans.copy()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "   id     label\n",
                        "0   1   England\n",
                        "0   2   England\n",
                        "1   3   England\n",
                        "0   4  Scotland\n",
                        "1   5  Scotland\n"
                    ]
                }
            ],
            "source": [
                "ans[\"index\"] += 1\n",
                "ans = ans.sort_values(by=[\"index\"])\n",
                "ans = ans.rename(columns={\"index\": \"id\"})\n",
                "ans = ans.drop(\"sencount\", axis=1)\n",
                "print(ans.head())\n",
                "ans.to_csv(\"idc_lang_pos.csv\",index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "# kfold = KFold(n_splits=5, shuffle=True, random_state=21)\n",
                "\n",
                "# for language in langs:\n",
                "#     print(\"working on:\", language)\n",
                "#     train = pd.read_csv(f\"corpus/train/{language}/{language}_noent.csv\")\n",
                "#     acc = []\n",
                "#     f1 = []\n",
                "#     for train_index, test_index in kfold.split(train):\n",
                "#         X_train, X_test = train.text[train_index], train.text[test_index]\n",
                "#         y_train, y_test = train.label[train_index], train.label[test_index]\n",
                "\n",
                "#         X = cvc.fit_transform(list(X_train) + list(X_test))\n",
                "#         X = log_entropy_weight(X)\n",
                "\n",
                "#         model.fit(X[:len(X_train)], y_train)\n",
                "#         res = model.predict(X[len(X_train):])\n",
                "#         acc.append(accuracy_score(res, y_test))\n",
                "#         f1.append(f1_score(res, y_test, average='weighted'))\n",
                "#     print(\"Acc:\", acc)\n",
                "#     print(\"F1: \", f1)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.10.2 64-bit",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.2"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
