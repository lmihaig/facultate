{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importuri necesare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citirea datelor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_path = ''\n",
    "train_data_df = pd.read_csv(os.path.join(data_path, 'train_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Distributia etichetelor in datele de antrenare \\n', train_data_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fiecare limba este reprezentata in mod egal\n",
    "train_data_df['language'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificarea etichetelor din string in int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercitiu: codificam etichetele / labels in valori cu numere intregi dela 0 la N\n",
    "# codificam etichetele / labels in valori cu numere intregi dela 0 la N\n",
    "# ar putea fi exercitiu la clasa ca sa se obisnuiasca cu dictionare\n",
    "etichete_unice = train_data_df['label'].unique()\n",
    "label2id = {}\n",
    "id2label = {}\n",
    "for idx, eticheta in enumerate(etichete_unice):\n",
    "    #TODO:\n",
    "    pass\n",
    "\n",
    "print(label2id)\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercitiu: aplicati dictionarul label2id peste toate etichetele din train\n",
    "labels = []\n",
    "#TODO:\n",
    "\n",
    "print(labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-procesarea datelor\n",
    "\n",
    "- extragem informațiile necesare din text\n",
    "- eliminăm semnele de punctuație\n",
    "- facem tokenizare (împărțire în cuvinte)\n",
    "- vezi tokenizatori in [NLTK](https://www.nltk.org/api/nltk.tokenize.html)\n",
    "- vezi tokenizatori in [spaCy](https://spacy.io/)\n",
    "- vezi tokenizatori precum [BPE, WordPiece, SentencePiece](https://huggingface.co/docs/transformers/tokenizer_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def proceseaza(text):\n",
    "    \"\"\"Functie simpla de procesare a textului.\n",
    "    Sugestii:\n",
    "    - cum procesati \\n new lines? (vezi functia strip())\n",
    "    - cum procesati empty token ''\n",
    "    - puteti introduce un tokenizator din nltk\n",
    "        https://www.nltk.org/api/nltk.tokenize.html\n",
    "    - puteti elimina sau pastra doar stop-words\n",
    "    \"\"\"\n",
    "    text = re.sub(\"[-.,;:!?\\\"\\'\\/()_*=`]\", \"\", text)\n",
    "    text_in_cuvinte = text.split(' ')\n",
    "    return text_in_cuvinte\n",
    "\n",
    "# cuvintele rezultate din functia de preprocesare:\n",
    "exemple_italian = train_data_df[train_data_df['language'] == 'italiano']\n",
    "print(exemple_italian)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_italian = exemple_italian['text'].iloc[0]\n",
    "print(proceseaza(text_italian)[:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicam functia de preprocesarea intregului set de date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercitiu: aplicati functia pe fiecare exemplu de antrenare\n",
    "data = []\n",
    "# TODO\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Împărțirea datelor în train, validare și test\n",
    "\n",
    "O împărțire brutală poate fi cea în funcție de ordinea în care apar datele.\n",
    "- cum poate afecat acest tip de împărțire rezultatele dacă datele sunt sortate după etichetă?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))\n",
    "\n",
    "# putem imparti datele de antrenare astfel:\n",
    "# 20% date de test din total \n",
    "# 15% date de validare din ce ramane dupa ce scoatem datele de test \n",
    "\n",
    "nr_test = int(20/100 * len(train_data_df))\n",
    "print(\"Nr de date de test: \", nr_test)\n",
    "\n",
    "nr_ramase = len(data) - nr_test\n",
    "nr_valid = int(15/100 * nr_ramase)\n",
    "print(\"Nr de date de validare: \", nr_valid)\n",
    "\n",
    "nr_train = nr_ramase - nr_valid\n",
    "print(\"Nr de date de antrenare: \", nr_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# facem impartirea in ordinea in care apar datele\n",
    "\n",
    "train_data = data[:nr_train]\n",
    "train_labels = labels[:nr_train]\n",
    "\n",
    "valid_data = data[nr_train : nr_train + nr_valid]\n",
    "valid_labels = labels[nr_train : nr_train + nr_valid]\n",
    "\n",
    "test_data = data[nr_train + nr_valid: ]\n",
    "test_labels = labels[nr_train + nr_valid:]\n",
    "\n",
    "\n",
    "print(f'Nr de exemple de train {len(train_labels)}')\n",
    "print(f'Nr de exemple de validare {len(valid_labels)}')\n",
    "print(f'Nr de exemple de test {len(test_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercitiu: vedeti cum sunt distribuite etichetele in train, valid, test (incercati bincount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "\n",
    "În cadrul acestei secțiuni vom face numărarea aparițiilor tuturor cuvintelor din datele noastre. Pentru o evaluare justă nu ar fi indicat să includem si cuvintele din datele de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# vom folosi o structura de date nativa din python\n",
    "# care functioneaza ca un dictionar care numara elementele hashable\n",
    "# dintr-o colectie\n",
    "ctr = Counter(['eggs', 'ham', 'eggs', 'egg'])\n",
    "print(ctr['bacon'])\n",
    "print(ctr['eggs'])\n",
    "\n",
    "ctr = Counter([(0,1), (0,0), (0,1)])\n",
    "print(ctr[(0,0)])\n",
    "print(ctr[(2,2)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctr = Counter(data[9234])\n",
    "print(ctr.keys())\n",
    "print(ctr.values())\n",
    "# care este cel mai frecvent cuvant in data[5]?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frecventa cuvintelor din setul de antrenare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "#TODO:\n",
    "\n",
    "print(counter.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reprezentarea datelor sub forma vectoriala\n",
    "\n",
    "- sa presupunem ca folosim primele N cuvinte non-nule ca caracteristici pentru fiecare text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "cuvinte_caracteristice = []\n",
    "for cuvant, frecventa in counter.most_common(N):\n",
    "    if cuvant.strip():\n",
    "        cuvinte_caracteristice.append(cuvant)\n",
    "print(cuvinte_caracteristice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fiecarui cuvant îi atribuim un id în funcție de poziția pe care se află\n",
    "- ordinea in care sunt stocate cheile intr-un dictionar este arbitrara \n",
    "- iar o lista este un obiect mutabil in care ordinea elementelor se poate schimba oricand\n",
    "- cel mai sigur este sa construim o mapare intre cuvinte si un id care sa reprezinte pozitia in vectorul de caracteristici\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {}\n",
    "id2word = {}\n",
    "for idx, cuv in enumerate(cuvinte_caracteristice):\n",
    "    word2id[cuv] = idx\n",
    "    id2word[idx] = cuv\n",
    "\n",
    "print(word2id)\n",
    "print(id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cand trebuie sa reprezentam un text sub forma vectoriala, ne raportam doar la cuvintele cheie pe care le folosim ca caracteristici\n",
    "- id-ul reprezinta pozitia in vector unde vom stoca aparitiile fiecarui cuvant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. numaram toate cuvintele din text\n",
    "ctr = Counter(train_data[1])\n",
    "\n",
    "# 2. prealocam un array care va reprezenta caracteristicel noastre\n",
    "features = np.zeros(len(cuvinte_caracteristice))\n",
    "\n",
    "# 3. umplem array-ul cu valorile obtinute din counter\n",
    "# fiecare pozitie din array trebuie sa reprezinte frecventa\n",
    "# aceluiasi cuvant in toate textele\n",
    "for idx in range(0, len(features)):\n",
    "    # obtinem cuvantul pentru pozitia idx\n",
    "    cuvant = id2word[idx]\n",
    "    # asignam valoarea corespunzatoare frecventei cuvantului\n",
    "    features[idx] = ctr[cuvant]\n",
    "\n",
    "print(features)\n",
    "print([id2word[idx] for idx in range(0, len(features))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punem totul cap la cap sub forma de functii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_most_common(how_many, texte_preprocesate):\n",
    "    \"\"\"Functie care returneaza cele mai frecvente cuvinte.\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    return cuvinte_caracteristice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_id_word_dicts(cuvinte_caracteristice):\n",
    "    '''Dictionarele word2id si id2word garanteaza o ordine\n",
    "    pentru cuvintele caracteristice.\n",
    "    '''\n",
    "    #TODO\n",
    "    return word2id, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(text_preprocesat, id2word):\n",
    "    \"\"\"Pentru un text preprocesat dat si un dictionar\n",
    "    care mapeaza pentru fiecare pozitie ce cuvant corespunde,\n",
    "    returneaza un vector care reprezinta\n",
    "    frecventele fiecarui cuvant.\n",
    "    \"\"\"\n",
    "    ctr = Counter(text_preprocesat)\n",
    "    features = np.zeros(len(id2word))\n",
    "    #TODO\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize_multi(texte, id2word):\n",
    "    '''Pentru un set de texte preprocesate si un dictionar\n",
    "    care mapeaza pentru fiecare pozitie ce cuvant corespunde,\n",
    "    returneaza matricea trasaturilor tuturor textelor.\n",
    "    '''\n",
    "    all_features = []\n",
    "    for text in texte:\n",
    "        all_features.append(featurize(text, id2word))\n",
    "    return np.array(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuvinte_caracteristice = count_most_common(30, train_data)\n",
    "print(len(cuvinte_caracteristice))\n",
    "word2id, id2word = build_id_word_dicts(cuvinte_caracteristice)\n",
    "\n",
    "X_train = featurize_multi(train_data, id2word)\n",
    "X_valid = featurize_multi(valid_data, id2word)\n",
    "X_test = featurize_multi(test_data, id2word)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "\n",
    "model = svm.LinearSVC(C=1)\n",
    "\n",
    "model.fit(X_train, train_labels)\n",
    "vpreds = model.predict(X_valid)\n",
    "tpreds = model.predict(X_test)\n",
    "\n",
    "print(accuracy_score(valid_labels, vpreds))\n",
    "print(accuracy_score(test_labels, tpreds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplu de lucru cu stop words in nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# luat in calcul cuvintele functionale, pot creste sau scadea acuratetea\n",
    "# in cazul de fata, cuvintele functionale (stop words) indica elemente de gramatica\n",
    "# care sunt specifice textelor de la Scotieni, Englezi, Irlandezi\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('french'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
