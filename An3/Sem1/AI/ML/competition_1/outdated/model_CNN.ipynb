{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pprint\n",
                "import pandas as pd\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
                "from sklearn.metrics import accuracy_score, f1_score\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras import preprocessing\n",
                "from tensorflow.keras.preprocessing.text import Tokenizer\n",
                "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
                "pp = pprint.PrettyPrinter(indent=4, sort_dicts=False)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "def preprocess(data):\n",
                "    data.text = data.text.apply(lambda x: x.lower())\n",
                "    data.text = data.text.replace(r'\\s+|\\\\n', '', regex=True)\n",
                "    return data\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "  language                                               text    label\n",
                        "0    dansk   dette er et fremragende initiativ, og jeg stø...  Ireland\n",
                        "1    dansk   hr. formand, jeg er sikker på, at alle her er...  Ireland\n",
                        "2    dansk   hr. formand, folk på den nordlige halvkugle t...  England\n",
                        "3    dansk   hr. formand, med forbehold af nogle få ændrin...  England\n",
                        "4    dansk   - hr. formand, jeg må protestere mod den lemf...  England\n",
                        "                                                text\n",
                        "0   hr. formand, selv om vi i høj grad sympatiser...\n",
                        "1   quiero dejar constancia de mi apoyo a este in...\n",
                        "2   . – el comercio ilegal de riñones humanos se ...\n",
                        "3   signor presidente, per introdurre una nota di...\n",
                        "4   jeg stemte for meddelelsen af decharge til fæ...\n"
                    ]
                }
            ],
            "source": [
                "train_data = pd.read_csv(\"data/train_data.csv\")\n",
                "train_data = preprocess(train_data)\n",
                "print(train_data.head())\n",
                "\n",
                "test_data = pd.read_csv(\"data/test_data.csv\")\n",
                "test_data = preprocess(test_data)\n",
                "print(test_data.head())\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2.10.0\n",
                        "/device:GPU:0\n"
                    ]
                }
            ],
            "source": [
                "print(tf.__version__)\n",
                "print(tf.test.gpu_device_name())\n",
                "\n",
                "train_data[\"label\"] = train_data[\"label\"].astype(\"category\")\n",
                "train_data[\"label\"] = train_data[\"label\"].cat.codes\n",
                "train_features, train_labels = train_data[\"text\"], tf.one_hot(train_data[\"label\"], 3)\n",
                "\n",
                "test_features = test_data[\"text\"]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "import nltk\n",
                "from nltk.tokenize import word_tokenize\n",
                "\n",
                "tokenized_train_features = [word_tokenize(each_train_text) for each_train_text in train_features]\n",
                "tokenized_test_features = [word_tokenize(each_test_text) for each_test_text in test_features]\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "from gensim.models import word2vec\n",
                "\n",
                "vector_size = 100\n",
                "\n",
                "w2v_model = word2vec.Word2Vec(\n",
                "    tokenized_train_features,\n",
                "    vector_size=vector_size,  # Dimensionality of the word vectors\n",
                "    window=20,\n",
                "    min_count=1,\n",
                "    sg=1  # 1 for skip-gram; otherwise CBOW\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "vocab_list = list(w2v_model.wv.key_to_index.keys())\n",
                "\n",
                "\n",
                "def remove_OOV_vocab(sample: list, list_vocab):\n",
                "    \"\"\" Takes in tokenized sample in the form of list \n",
                "    and the vocabulary list and removes tokens from sample\n",
                "    that are not in the vocabulary list\"\"\"\n",
                "    in_vocab_sample = []\n",
                "    for each_token in sample:\n",
                "        if each_token in list_vocab:\n",
                "            in_vocab_sample.append(each_token)\n",
                "    return in_vocab_sample\n",
                "\n",
                "\n",
                "tokenized_test_features = [remove_OOV_vocab(each_test_sample, vocab_list) for each_test_sample in tokenized_test_features]\n",
                "\n",
                "\n",
                "vocab = w2v_model.wv.key_to_index.keys()\n",
                "embedding_matrix = w2v_model.wv[vocab]\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
                "\n",
                "max_seq_len = 100\n",
                "\n",
                "\n",
                "def w2v_indexed_token_sequences(w2v_model, list_features):\n",
                "    indexed_features = []\n",
                "    for each_seq in list_features:\n",
                "        list_token_indices = []\n",
                "        for each_token in each_seq:\n",
                "            try:\n",
                "                list_token_indices.append(w2v_model.wv.key_to_index[each_token])\n",
                "            except KeyError as e:\n",
                "                continue\n",
                "        indexed_features.append(list_token_indices)\n",
                "    return indexed_features\n",
                "\n",
                "\n",
                "indexed_train_features = w2v_indexed_token_sequences(w2v_model, tokenized_train_features)\n",
                "indexed_test_features = w2v_indexed_token_sequences(w2v_model, tokenized_test_features)\n",
                "\n",
                "padded_train = pad_sequences(indexed_train_features, padding='post', maxlen=max_seq_len, truncating='post')\n",
                "padded_test = pad_sequences(indexed_test_features, padding='post', maxlen=max_seq_len, truncating='post')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 40,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1/10\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\lmg\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:264: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "871/871 [==============================] - ETA: 0s - loss: 0.9775 - accuracy: 0.5503\n",
                        "Epoch 1: val_loss improved from inf to 1.05458, saving model to models\\lstm_with_w2v.hdf5\n",
                        "871/871 [==============================] - 61s 70ms/step - loss: 0.9775 - accuracy: 0.5503 - val_loss: 1.0546 - val_accuracy: 0.4613\n",
                        "Epoch 2/10\n",
                        "871/871 [==============================] - ETA: 0s - loss: 0.9336 - accuracy: 0.5793\n",
                        "Epoch 2: val_loss improved from 1.05458 to 1.01935, saving model to models\\lstm_with_w2v.hdf5\n",
                        "871/871 [==============================] - 61s 70ms/step - loss: 0.9336 - accuracy: 0.5793 - val_loss: 1.0194 - val_accuracy: 0.5118\n",
                        "Epoch 3/10\n",
                        "871/871 [==============================] - ETA: 0s - loss: 0.7243 - accuracy: 0.7027\n",
                        "Epoch 3: val_loss did not improve from 1.01935\n",
                        "871/871 [==============================] - 60s 69ms/step - loss: 0.7243 - accuracy: 0.7027 - val_loss: 1.1893 - val_accuracy: 0.4750\n",
                        "Epoch 4/10\n",
                        "871/871 [==============================] - ETA: 0s - loss: 0.4727 - accuracy: 0.8158\n",
                        "Epoch 4: val_loss did not improve from 1.01935\n",
                        "871/871 [==============================] - 60s 69ms/step - loss: 0.4727 - accuracy: 0.8158 - val_loss: 1.1195 - val_accuracy: 0.5115\n",
                        "Epoch 5/10\n",
                        "871/871 [==============================] - ETA: 0s - loss: 0.2791 - accuracy: 0.8960\n",
                        "Epoch 5: val_loss did not improve from 1.01935\n",
                        "871/871 [==============================] - 60s 69ms/step - loss: 0.2791 - accuracy: 0.8960 - val_loss: 1.1793 - val_accuracy: 0.5358\n",
                        "Epoch 6/10\n",
                        "871/871 [==============================] - ETA: 0s - loss: 0.1770 - accuracy: 0.9343Restoring model weights from the end of the best epoch: 2.\n",
                        "\n",
                        "Epoch 6: val_loss did not improve from 1.01935\n",
                        "871/871 [==============================] - 62s 70ms/step - loss: 0.1770 - accuracy: 0.9343 - val_loss: 1.3962 - val_accuracy: 0.5078\n",
                        "Epoch 6: early stopping\n"
                    ]
                }
            ],
            "source": [
                "import tensorflow as tf\n",
                "import tensorflow.keras as keras\n",
                "from tensorflow.keras import Sequential\n",
                "from tensorflow.keras.layers import Dense, Embedding, Dropout, LSTM\n",
                "\n",
                "\n",
                "def get_model():\n",
                "    model = Sequential()\n",
                "    model.add(\n",
                "        Embedding(input_dim=259925,\n",
                "                  output_dim=vector_size,\n",
                "                  weights=[embedding_matrix],\n",
                "                  input_length=max_seq_len))\n",
                "    model.add(Dropout(0.5))\n",
                "    model.add(LSTM(max_seq_len, return_sequences=True))\n",
                "    model.add(LSTM(15))\n",
                "    model.add(Dense(3, activation='softmax'))\n",
                "    return model\n",
                "\n",
                "\n",
                "# Adding callbacks for best model checkpoint\n",
                "callbacks = [\n",
                "    keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
                "                                  patience=4,\n",
                "                                  verbose=1,\n",
                "                                  restore_best_weights=True),\n",
                "    keras.callbacks.ModelCheckpoint(filepath='models/lstm_with_w2v.hdf5',\n",
                "                                    verbose=1,\n",
                "                                    save_best_only=True)\n",
                "]\n",
                "\n",
                "model = get_model()\n",
                "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
                "tf.config.run_functions_eagerly(True)\n",
                "\n",
                "# storing model training details to analyze later\n",
                "history = model.fit(padded_train,\n",
                "                    train_labels,\n",
                "                    validation_split=0.33,\n",
                "                    callbacks=callbacks,\n",
                "                    epochs=10)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "434/434 [==============================] - 9s 21ms/step\n"
                    ]
                }
            ],
            "source": [
                "model_with_w2v = keras.models.load_model('models/lstm_with_w2v.hdf5')\n",
                "y_pred_one_hot_encoded = (model_with_w2v.predict(padded_test) > 0.5).astype(\"int32\")\n",
                "y_pred_test = np.array(tf.argmax(y_pred_one_hot_encoded, axis=1))\n",
                "mappings = {0: 'England', 1: 'Ireland', 2: 'Scotland'}\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 42,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "id\n",
                            "1    England\n",
                            "2    England\n",
                            "3    England\n",
                            "4    England\n",
                            "5    England\n",
                            "Name: label, dtype: object"
                        ]
                    },
                    "execution_count": 42,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "final_data = {\"id\": test_data.index+1, \"label\": y_pred_test}\n",
                "\n",
                "submission = pd.DataFrame(data=final_data).set_index(\"id\")\n",
                "submission = submission.label.apply(lambda x: mappings[x])\n",
                "submission.to_csv(\"submissions/submission_CNN.csv\")\n",
                "submission.head()\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.10.2 64-bit",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.2"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
