{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "pp = pprint.PrettyPrinter(indent=4, sort_dicts=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data.text = data.text.apply(lambda x: x.lower())\n",
    "    data.text = data.text.replace(r'\\s+|\\\\n', '', regex=True)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"data/train_data.csv\")\n",
    "train_data = preprocess(train_data)\n",
    "print(train_data.head())\n",
    "\n",
    "test_data = pd.read_csv(\"data/test_data.csv\")\n",
    "test_data = preprocess(test_data)\n",
    "print(test_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.test.gpu_device_name())\n",
    "\n",
    "train_data[\"label\"] = train_data[\"label\"].astype(\"category\")\n",
    "train_data[\"label\"] = train_data[\"label\"].cat.codes\n",
    "train_features, train_labels = train_data[\"text\"], tf.one_hot(train_data[\"label\"], 3)\n",
    "\n",
    "test_features = test_data[\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_train_features = [word_tokenize(each_train_text) for each_train_text in train_features]\n",
    "tokenized_test_features = [word_tokenize(each_test_text) for each_test_text in test_features]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "vector_size = 100\n",
    "\n",
    "w2v_model = word2vec.Word2Vec(\n",
    "    tokenized_train_features,\n",
    "    vector_size=vector_size,  # Dimensionality of the word vectors\n",
    "    window=20,\n",
    "    min_count=1,\n",
    "    sg=1  # 1 for skip-gram; otherwise CBOW\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(w2v_model.wv.key_to_index.keys())\n",
    "\n",
    "\n",
    "def remove_OOV_vocab(sample: list, list_vocab):\n",
    "    \"\"\" Takes in tokenized sample in the form of list \n",
    "    and the vocabulary list and removes tokens from sample\n",
    "    that are not in the vocabulary list\"\"\"\n",
    "    in_vocab_sample = []\n",
    "    for each_token in sample:\n",
    "        if each_token in list_vocab:\n",
    "            in_vocab_sample.append(each_token)\n",
    "    return in_vocab_sample\n",
    "\n",
    "\n",
    "tokenized_test_features = [remove_OOV_vocab(each_test_sample, vocab_list) for each_test_sample in tokenized_test_features]\n",
    "\n",
    "\n",
    "vocab = w2v_model.wv.key_to_index.keys()\n",
    "embedding_matrix = w2v_model.wv[vocab]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_seq_len = 100\n",
    "\n",
    "\n",
    "def w2v_indexed_token_sequences(w2v_model, list_features):\n",
    "    indexed_features = []\n",
    "    for each_seq in list_features:\n",
    "        list_token_indices = []\n",
    "        for each_token in each_seq:\n",
    "            try:\n",
    "                list_token_indices.append(w2v_model.wv.key_to_index[each_token])\n",
    "            except KeyError as e:\n",
    "                continue\n",
    "        indexed_features.append(list_token_indices)\n",
    "    return indexed_features\n",
    "\n",
    "\n",
    "indexed_train_features = w2v_indexed_token_sequences(w2v_model, tokenized_train_features)\n",
    "indexed_test_features = w2v_indexed_token_sequences(w2v_model, tokenized_test_features)\n",
    "\n",
    "padded_train = pad_sequences(indexed_train_features, padding='post', maxlen=max_seq_len, truncating='post')\n",
    "padded_test = pad_sequences(indexed_test_features, padding='post', maxlen=max_seq_len, truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Dropout, LSTM\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Embedding(input_dim=259925,\n",
    "                  output_dim=vector_size,\n",
    "                  weights=[embedding_matrix],\n",
    "                  input_length=max_seq_len))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(max_seq_len, return_sequences=True))\n",
    "    model.add(LSTM(15))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "\n",
    "# Adding callbacks for best model checkpoint\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                  patience=4,\n",
    "                                  verbose=1,\n",
    "                                  restore_best_weights=True),\n",
    "    keras.callbacks.ModelCheckpoint(filepath='models/lstm_with_w2v.hdf5',\n",
    "                                    verbose=1,\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "\n",
    "model = get_model()\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# storing model training details to analyze later\n",
    "history = model.fit(padded_train,\n",
    "                    train_labels,\n",
    "                    validation_split=0.33,\n",
    "                    callbacks=callbacks,\n",
    "                    epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_w2v = keras.models.load_model('models/lstm_with_w2v.hdf5')\n",
    "y_pred_one_hot_encoded = (model_with_w2v.predict(padded_test) > 0.5).astype(\"int32\")\n",
    "y_pred_test = np.array(tf.argmax(y_pred_one_hot_encoded, axis=1))\n",
    "mappings = {0: 'England', 1: 'Ireland', 2: 'Scotland'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = {\"id\": test_data.index+1, \"label\": y_pred_test}\n",
    "\n",
    "submission = pd.DataFrame(data=final_data).set_index(\"id\")\n",
    "submission = submission.label.apply(lambda x: mappings[x])\n",
    "submission.to_csv(\"submissions/submission_CNN.csv\")\n",
    "submission.head()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
