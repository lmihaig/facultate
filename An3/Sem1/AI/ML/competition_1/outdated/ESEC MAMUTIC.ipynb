{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect_langs\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_validate, KFold\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, make_scorer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer, TfidfTransformer\n",
    "import advertools as adv\n",
    "pp = pprint.PrettyPrinter(indent=4, sort_dicts=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data into dataframes and doing light preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data.text = data.text.apply(lambda x: x.lower())\n",
    "    data.text = data.text.replace(r\"\\s+|\\\\n\", \" \", regex=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "train_data = pd.read_csv(\"data/train_data.csv\")\n",
    "train_data = preprocess(train_data)\n",
    "print(train_data.head())\n",
    "\n",
    "\n",
    "\n",
    "test_data = pd.read_csv(\"data/test_data.csv\")\n",
    "test_data = preprocess(test_data)\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "mappings = []\n",
    "for col in [\"language\", \"label\"]:\n",
    "    train_data[col] = le.fit_transform(train_data[col])\n",
    "    mappings.append(dict(zip(le.transform(le.classes_), le.classes_)))\n",
    "\n",
    "print(mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "for key in [\"danish\", \"german\", \"dutch\", \"italian\", \"spanish\"]:\n",
    "    stop_words+= list(adv.stopwords[key])\n",
    "\n",
    "# count_vec = TfidfVectorizer(stop_words=stop_words)\n",
    "count_vec = CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "train_bow = count_vec.fit_transform(train_data.text)\n",
    "test_bow = count_vec.transform(test_data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# language prediction\n",
    "model = ComplementNB().fit(train_bow, train_data.language)\n",
    "predicted_langs = model.predict(test_bow)\n",
    "predicted_langs = pd.Series(predicted_langs, name=\"language\")\n",
    "test_data = pd.concat([test_data, predicted_langs], axis=1)\n",
    "test_data.language.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ComplementNB().fit(train_bow, train_data.label)\n",
    "# predicted_labels = model.predict(test_bow)\n",
    "# print(predicted_labels)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "train_bows = defaultdict(int)\n",
    "test_bows = defaultdict(int)\n",
    "models = defaultdict(int)\n",
    "predicted_labels = [0] * len(train_data)\n",
    "\n",
    "for language in mappings[0].keys():\n",
    "    train_bows[language] = count_vec.fit_transform(train_data[train_data.language == language].text)\n",
    "    test_bows[language] = count_vec.transform(test_data[test_data.language == language].text)\n",
    "\n",
    "\n",
    "for language in mappings[0].keys():\n",
    "    models[language] = ComplementNB().fit(train_bows[language], train_data[train_data.language == language].label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = [models[i].predict(test_bows[i]) for i in mappings[0].keys()]\n",
    "predicted_labels = np.concatenate(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the predicted data to a submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = {\"id\": test_data.index+1, \"label\": predicted_labels}\n",
    "\n",
    "submission = pd.DataFrame(data=final_data).set_index(\"id\")\n",
    "submission = submission.label.apply(lambda x: mappings[1][x])\n",
    "submission.to_csv(\"submission_NB.csv\")\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(_X, _y, models, _cv=5):\n",
    "    \n",
    "    acc = []\n",
    "    _scoring = {'accuracy': make_scorer(accuracy_score),\n",
    "                'precision': make_scorer(precision_score, average='macro'),\n",
    "                'recall': make_scorer(recall_score, average='macro'),\n",
    "                'f1': make_scorer(f1_score, average='macro')\n",
    "    }\n",
    "\n",
    "    kfold = KFold(n_splits=10, shuffle=True, random_state=69420)\n",
    "    for model in models:\n",
    "        results = cross_validate(estimator=model,\n",
    "                                 X=_X,\n",
    "                                 y=_y,\n",
    "                                 cv=kfold,\n",
    "                                 scoring=_scoring,\n",
    "                                 return_train_score=True)\n",
    "                                 \n",
    "        pp.pprint({\"Model\": type(model).__name__,\n",
    "                \"Training Accuracy scores\": results['train_accuracy'],\n",
    "                \"Mean Training Accuracy\": results['train_accuracy'].mean()*100,\n",
    "                \"Training Precision scores\": results['train_precision'],\n",
    "                \"Mean Training Precision\": results['train_precision'].mean(),\n",
    "                \"Training Recall scores\": results['train_recall'],\n",
    "                \"Mean Training Recall\": results['train_recall'].mean(),\n",
    "                \"Training F1 scores\": results['train_f1'],\n",
    "                \"Mean Training F1 Score\": results['train_f1'].mean(),\n",
    "                \"Validation Accuracy scores\": results['test_accuracy'],\n",
    "                \"Mean Validation Accuracy\": results['test_accuracy'].mean()*100,\n",
    "                \"Validation Precision scores\": results['test_precision'],\n",
    "                \"Mean Validation Precision\": results['test_precision'].mean(),\n",
    "                \"Validation Recall scores\": results['test_recall'],\n",
    "                \"Mean Validation Recall\": results['test_recall'].mean(),\n",
    "                \"Validation F1 scores\": results['test_f1'],\n",
    "                \"Mean Validation F1 Score\": results['test_f1'].mean()\n",
    "                   })\n",
    "        acc.append(results['test_f1'].mean()*100)\n",
    "    return acc\n",
    "\n",
    "\n",
    "models = [ComplementNB(), BernoulliNB(), MultinomialNB()]\n",
    "print(compare_models(train_bow, train_data.label, models))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_bow\n",
    "y = train_data.label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "\n",
    "model = ComplementNB().fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "print('F1 score: ', f1_score(y_test, y_pred, average=\"macro\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
