{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_validate, GridSearchCV\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer, precision_score, recall_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import advertools as adv\n",
    "pp = pprint.PrettyPrinter(indent=4, sort_dicts=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"corpus/nolang_dif/train_data.csv\",index_col=\"index\")\n",
    "print(train_data.head())\n",
    "\n",
    "\n",
    "test_data = pd.read_csv(\"corpus/nolang_dif/test_data.csv\", index_col=\"index\")\n",
    "print(test_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(\"language\", axis=1)\n",
    "train_data = train_data[pd.notnull(train_data[\"text\"])]\n",
    "train_data[\"category_id\"] = train_data[\"label\"].astype(\"category\")\n",
    "train_data[\"category_id\"] = train_data[\"category_id\"].cat.codes\n",
    "\n",
    "train_data.head()\n",
    "\n",
    "category_id_df = train_data[[\"label\", \"category_id\"]].drop_duplicates().sort_values(\"category_id\")\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[[\"category_id\", \"label\"]].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "train_data.groupby(\"label\").text.count().plot.bar(ylim=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "train_sent_len = np.array(train_data.text.apply(lambda x: len(x)))\n",
    "test_sent_len = np.array(test_data.text.apply(lambda x: len(x)))\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = 10, 5\n",
    "plt.hist(train_sent_len[np.where(train_sent_len < 7500)], alpha=0.5, label=\"train\")\n",
    "plt.hist(test_sent_len[np.where(test_sent_len < 7500)], alpha=0.5, label=\"test\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Number of chars in paragraph\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import advertools as adv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "stop_words = []\n",
    "for key in [\"danish\", \"german\", \"dutch\", \"italian\", \"spanish\"]:\n",
    "    stop_words += list(adv.stopwords[key])\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm=\"l2\", encoding=\"latin-1\", ngram_range=(1, 2), stop_words=stop_words)\n",
    "\n",
    "features = tfidf.fit_transform(train_data.text)\n",
    "labels = train_data.category_id\n",
    "features.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "N = 2\n",
    "for dialect, category_id in sorted(category_to_id.items()):\n",
    "  features_chi2 = chi2(features, labels == category_id)\n",
    "  indices = np.argsort(features_chi2[0])\n",
    "  feature_names = np.array(tfidf.get_feature_names_out())[indices]\n",
    "  unigrams = [v for v in feature_names if len(v.split(\" \")) == 1]\n",
    "  bigrams = [v for v in feature_names if len(v.split(\" \")) == 2]\n",
    "  # trigrams = [v for v in feature_names if len(v.split(\" \")) == 3]\n",
    "  print(\"# '{}':\".format(dialect))\n",
    "  print(\"  . Most correlated unigrams:\\n. {}\".format(\"\\n. \".join(unigrams[-N:])))\n",
    "  print(\"  . Most correlated bigrams:\\n. {}\".format(\"\\n. \".join(bigrams[-N:])))\n",
    "  # print(\"  . Most correlated trigrams:\\n. {}\".format(\"\\n. \".join(trigrams[-N:])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "models = [\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    LinearSVC(max_iter=5000),\n",
    "    ComplementNB(),\n",
    "    LogisticRegression(random_state=0, max_iter=5000),\n",
    "\n",
    "    LinearSVC(max_iter=5000, random_state=21, C=1.5,  penalty=\"l1\", dual=False, class_weight=\"balanced\"),\n",
    "    \n",
    "]\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=69420)\n",
    "entries = []\n",
    "_scoring = make_scorer(f1_score, average=\"macro\")\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "  model_name = f\"{model.__class__.__name__}_{i}\"\n",
    "  scores = cross_val_score(model, features, labels, scoring=_scoring, cv=kfold)\n",
    "  for fold_idx, score in enumerate(scores):\n",
    "    entries.append((model_name, fold_idx, score))\n",
    "\n",
    "cv_df = pd.DataFrame(entries, columns=[\"model_name\", \"fold_idx\", \"score\"])\n",
    "sns.boxplot(x=\"model_name\", y=\"score\", data=cv_df)\n",
    "sns.stripplot(x=\"model_name\", y=\"score\", data=cv_df,\n",
    "              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "plt.show()\n",
    "print(cv_df.groupby(\"model_name\").score.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC()\n",
    "params = {  \"penalty\": [\"l1\",\"l2\"],\n",
    "            \"dual\": [True, False],\n",
    "            \"max_iter\": [6009],\n",
    "            \"C\": [1,5],\n",
    "            \"loss\": ['hinge', 'squared_hinge'],\n",
    "            \"multi_class\": [\"ovr\", \"crammer_singer\"],\n",
    "            \"class_weight\": [\"balanced\", None],\n",
    "            \"random_state\": [21]\n",
    "} \n",
    "clf = GridSearchCV(model, params, scoring=_scoring, cv=kfold, return_train_score=True, n_jobs=-1, verbose=2)\n",
    "clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
