{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "import numpy as np\n",
                "import pprint\n",
                "import pandas as pd\n",
                "from sklearn.svm import LinearSVC\n",
                "from sklearn.model_selection import train_test_split, KFold, cross_validate\n",
                "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
                "from sklearn.metrics import accuracy_score, f1_score, make_scorer, precision_score, recall_score\n",
                "from sklearn import preprocessing\n",
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "import advertools as adv\n",
                "from matplotlib import pyplot as plt\n",
                "from sklearn.metrics import confusion_matrix, classification_report\n",
                "from sklearn.model_selection import cross_val_score,  cross_val_predict\n",
                "import seaborn as sns\n",
                "from imblearn.under_sampling import RandomUnderSampler\n",
                "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
                "pp = pprint.PrettyPrinter(indent=4, sort_dicts=False)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": [
                "def log_entropy_weight(matrix):\n",
                "    if type(matrix) is not np.ndarray:\n",
                "        matrix = matrix.toarray()\n",
                "    normalized = matrix / (1 + np.sum(matrix, axis=0))\n",
                "    nr_docs, _ = matrix.shape\n",
                "    '''\n",
                "        g_i = 1 + sum     p_ij * log(p_ij + 1)   \n",
                "                 j=1,N  ------------------------\n",
                "                               log(N)                              \n",
                "    '''\n",
                "    entropy = 1 + np.sum(np.multiply(normalized, np.log(normalized + 1)), axis=0)/np.log(nr_docs)\n",
                "    '''\n",
                "        logent_ij = gi * log(tf_ij + 1)\n",
                "    '''\n",
                "    log_ent = entropy * np.log(matrix + 1)\n",
                "    return log_ent\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.svm import LinearSVC\n",
                "\n",
                "\n",
                "langs = [\"nl\",\"da\",\"de\",\"it\",\"es\",]\n",
                "stop_words = []\n",
                "for key in [\"danish\", \"german\", \"dutch\", \"italian\", \"spanish\"]:\n",
                "    stop_words += list(adv.stopwords[key])\n",
                "\n",
                "model = LogisticRegression(penalty='l2',\n",
                "                           dual=False,\n",
                "                           max_iter=10000,\n",
                "                           tol=0.0001,\n",
                "                           solver='liblinear',\n",
                "                           C=1,\n",
                "                           fit_intercept=True,\n",
                "                           intercept_scaling=1.0,\n",
                "                           class_weight=None,\n",
                "                           random_state=1)\n",
                "# model = LinearSVC(max_iter=5000, random_state=21, C=1,  penalty=\"l1\", dual=False, class_weight=\"balanced\")\n",
                "\n",
                "cvc = CountVectorizer(max_features=2000,\n",
                "                      strip_accents='unicode',\n",
                "                      ngram_range=(1, 5))\n",
                "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', ngram_range=(1, 5), stop_words=stop_words)\n",
                "ans = pd.DataFrame()\n",
                "\n",
                "\n",
                "def simpleModel(X_train, y_train, X_test):\n",
                "    print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\n",
                "    X = tfidf.fit_transform(list(X_train) + list(X_test))\n",
                "    \n",
                "    # X = cvc.fit_transform(list(X_train) + list(X_test))\n",
                "    # X = log_entropy_weight(X)\n",
                "    model.fit(X[:len(X_train)], y_train)\n",
                "    res = model.predict(X[len(X_train):])\n",
                "    return res\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": [
                "# for language in langs:\n",
                "#     print(\"working on:\", language)\n",
                "#     all_train_data = pd.read_csv(f\"../corpus/train/{language}/{language}_pos.csv\")\n",
                "#     all_test_data = pd.read_csv(f\"../corpus/test/{language}/{language}_pos.csv\")\n",
                "\n",
                "#     all_train_data[\"sencount\"] = all_train_data.text.str.count(\"SENTSEP\")\n",
                "#     all_test_data[\"sencount\"] = all_test_data.text.str.count(\"SENTSEP\")\n",
                "#     max_sen_count = max(all_train_data[\"sencount\"].max(), all_test_data[\"sencount\"].max())\n",
                "#     for l, r in [(0, 1000)]:\n",
                "#         print(f\"sentences of {l} <= len < {r}\")\n",
                "#         train_data = all_train_data.loc[all_train_data.sencount.between(l, r, inclusive=\"left\")].copy()\n",
                "#         test_data = all_test_data.loc[all_test_data.sencount.between(l, r, inclusive=\"left\")].copy()\n",
                "\n",
                "#         res = simpleModel(train_data.text, train_data.label, test_data.text)\n",
                "        \n",
                "#         test_data[\"label\"] = res\n",
                "#         test_data = test_data.drop(\"text\", axis=1)\n",
                "#         ans = pd.concat([ans, test_data])\n",
                "# results = ans.copy()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ans[\"index\"] += 1\n",
                "# ans = ans.sort_values(by=[\"index\"])\n",
                "# ans = ans.rename(columns={\"index\": \"id\"})\n",
                "# ans = ans.drop(\"sencount\", axis=1)\n",
                "# print(ans.head())\n",
                "# ans.to_csv(\"../submissions/beepboop.csv\",index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "working on: nl\n",
                        "Train size: 6650, Test size: 1663\n",
                        "Train size: 6650, Test size: 1663\n",
                        "Train size: 6650, Test size: 1663\n",
                        "Train size: 6651, Test size: 1662\n",
                        "Train size: 6651, Test size: 1662\n",
                        "Acc: [0.5646422128683103, 0.5923030667468431, 0.571256764882742, 0.5860409145607701, 0.5896510228640193]\n",
                        "F1:  [0.661508384536065, 0.6940999123775468, 0.6610206028383783, 0.6785856462364872, 0.6794223317949042]\n",
                        "working on: da\n",
                        "Train size: 6651, Test size: 1663\n",
                        "Train size: 6651, Test size: 1663\n",
                        "Train size: 6651, Test size: 1663\n",
                        "Train size: 6651, Test size: 1663\n",
                        "Train size: 6652, Test size: 1662\n",
                        "Acc: [0.5670475045099218, 0.5917017438364401, 0.5598316295850871, 0.5796752856283824, 0.5998796630565584]\n",
                        "F1:  [0.6582690238292602, 0.6723787389940379, 0.6463430710358398, 0.6616516633241912, 0.6755742763309389]\n",
                        "working on: de\n",
                        "Train size: 6650, Test size: 1663\n",
                        "Train size: 6650, Test size: 1663\n",
                        "Train size: 6650, Test size: 1663\n",
                        "Train size: 6651, Test size: 1662\n",
                        "Train size: 6651, Test size: 1662\n",
                        "Acc: [0.5742633794347565, 0.5862898376428142, 0.571858087793145, 0.5950661853188929, 0.6064981949458483]\n",
                        "F1:  [0.6657282366853886, 0.6773079789546839, 0.6614020057538024, 0.6778366441641862, 0.6830627451736757]\n",
                        "working on: it\n",
                        "Train size: 6651, Test size: 1663\n",
                        "Train size: 6651, Test size: 1663\n",
                        "Train size: 6651, Test size: 1663\n",
                        "Train size: 6651, Test size: 1663\n",
                        "Train size: 6652, Test size: 1662\n",
                        "Acc: [0.5694527961515333, 0.5796752856283824, 0.5652435357787132, 0.5935057125676488, 0.6070998796630566]\n",
                        "F1:  [0.6629579986016925, 0.6687497500894268, 0.6551798645460474, 0.6799243245262567, 0.687177877115826]\n",
                        "working on: es\n",
                        "Train size: 6651, Test size: 1663\n",
                        "Train size: 6651, Test size: 1663\n",
                        "Train size: 6651, Test size: 1663\n",
                        "Train size: 6651, Test size: 1663\n",
                        "Train size: 6652, Test size: 1662\n",
                        "Acc: [0.571858087793145, 0.571858087793145, 0.5814792543595911, 0.5923030667468431, 0.5950661853188929]\n",
                        "F1:  [0.6649611545937522, 0.6624925743663043, 0.6694740155165652, 0.6766288632485283, 0.6782700565528346]\n"
                    ]
                }
            ],
            "source": [
                "kfold = KFold(n_splits=5, shuffle=True, random_state=21)\n",
                "\n",
                "# # LANGUAGE\n",
                "# for language in langs:\n",
                "#     print(\"working on:\", language)\n",
                "#     train = pd.read_csv(f\"../corpus/train/{language}/{language}_pos.csv\")\n",
                "#     acc = []\n",
                "#     f1 = []\n",
                "#     for train_index, test_index in kfold.split(train):\n",
                "#         X_train, X_test = train.text[train_index], train.text[test_index]\n",
                "#         y_train, y_test = list(train.label[train_index]), list(train.label[test_index])\n",
                "\n",
                "#         res = simpleModel(X_train, y_train, X_test)\n",
                "\n",
                "#         acc.append(accuracy_score(res, y_test))\n",
                "#         f1.append(f1_score(res, y_test, average=\"weighted\"))\n",
                "\n",
                "#     print(\"Acc:\", acc)\n",
                "#     print(\"F1: \", f1)\n",
                "\n",
                "\n",
                "# # NO LANG\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# nolang svm tf-idf = ~0.70\n",
                "\n",
                "# regresion log-entropy noent = ~0.6\n",
                "# regresion log-entropy pos = ~0.5\n",
                "# svm log-entropy pos = ~ failed to converge\n",
                "# svm tf-idf pos = ~ failed to converge\n",
                "# svm log-entropy noent = ~ failed to converge\n",
                "# svm tf-idf noent = ~ 0.6\n",
                "# regresion tf-idf noent =  ~ 0.61\n",
                "# regresion tf-idf pos = ~0.59\n",
                "\n",
                "# nolang regression log-entropy noent = ~ 0.58\n",
                "# nolang regression log-entropy pos = ~ 0.56\n",
                "# nolang svm log-entropy noent = ~\n",
                "# nolang svm log-entropy pos = ~ failed to converge\n",
                "# nolang svm tf-idf noent = ~\n",
                "# nolang svm tf-idf pos = ~\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.10.2 64-bit",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.2"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
